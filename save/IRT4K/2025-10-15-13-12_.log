2025-10-15 13:12:33,816 data:
  batch_size: 4
  name: IRT4K
finetune:
  ckpt_path: None
model:
  ckpt_path: None
  default_scale: True
  eps: 0.0001
  first_stage:
    ckpt_path: model/model-30.pt
    ddconfig:
      attn_resolutions: []
      ch: 128
      ch_mult: [1, 2, 4]
      double_z: True
      dropout: 0.0
      in_channels: 1
      num_res_blocks: 2
      out_ch: 1
      resolution: [320, 320]
      z_channels: 3
    embed_dim: 3
    lossconfig:
      disc_in_channels: 1
      disc_start: 50001
      disc_weight: 0.5
      kl_weight: 1e-06
  ignore_keys: []
  image_size: [320, 320]
  input_keys: ['image', 'cond']
  loss_type: l2
  model_name: cond_unet
  model_type: const_sde
  objective: pred_KC
  only_model: False
  perceptual_weight: 0
  sampling_timesteps: 50
  scale_by_softsign: False
  scale_by_std: True
  scale_factor: 0.3
  start_dist: normal
  timesteps: 1000
  train_sample: -1
  unet:
    DPMCARK: False
    channels: 3
    cond_dim: 128
    cond_dim_mults: [2, 4]
    cond_feature_size: [80, 80]
    cond_in_dim: 3
    cond_net: swin
    cond_pe: False
    dim: 128
    dim_mults: [1, 2, 4, 4]
    fix_bb: False
    fourier_scale: 16
    input_size: [80, 80]
    num_pos_feats: 128
    out_mul: 1
    window_sizes1: [[8, 8], [4, 4], [2, 2], [1, 1]]
    window_sizes2: [[8, 8], [4, 4], [2, 2], [1, 1]]
  use_disloss: True
  weighting_loss: True
trainer:
  amp: False
  ema_update_after_step: 10000
  ema_update_every: 10
  enable_resume: False
  fp16: False
  gradient_accumulate_every: 1
  log_freq: 500
  lr: 1e-05
  min_lr: 5e-06
  results_folder: ./save/IRT4K
  resume_milestone: 0
  save_and_sample_every: 500
  test_before: True
  train_num_steps: 5000
2025-10-15 13:12:38,935 [Train Step] 0/5000: loss_simple: 3.12696, loss_vlb: 0.00000, total_loss: 3.12696, lr: 0.00001, 
2025-10-15 13:15:53,059 [Train Step] 500/5000: loss_simple: 2.69749, loss_vlb: 0.00000, total_loss: 2.69749, lr: 0.00001, 
